{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI methods in a nutshell",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metasir/AI-demo-notebooks/blob/master/AI_methods_in_a_nutshell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhfF8bX5DvXT",
        "colab_type": "toc"
      },
      "source": [
        ">[AI Methods and Techniques](#scrollTo=Fis92oy3kR-V&uniqifier=1)\n",
        "\n",
        ">[GOFAI](#scrollTo=T8AjqfdNIDg9&uniqifier=1)\n",
        "\n",
        ">>[A* Search](#scrollTo=pYbZpokpKzRi&uniqifier=1)\n",
        "\n",
        ">>[Reinforcement Learning](#scrollTo=-oA779Zj9-AG&uniqifier=1)\n",
        "\n",
        ">[Machine Learning](#scrollTo=Ky5Hux3VWjDz&uniqifier=1)\n",
        "\n",
        ">[TensorFlow Playground](#scrollTo=PyDAqkyvmKeA&uniqifier=1)\n",
        "\n",
        ">>[Exercises](#scrollTo=i9KGJsd_rf0K&uniqifier=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CyCahdWLMSY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Imports]\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import urllib.request\n",
        "import ipywidgets as widgets\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output\n",
        "from google.colab import output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fis92oy3kR-V",
        "colab_type": "text"
      },
      "source": [
        "# AI Methods and Techniques\n",
        "\n",
        "In this notebook we'll briefly look at various methods of dealing with hard problems and big data-sets. AI tends to fall into two main types: \"Good Old-Fashioned AI\" (or GOFAI), and \"Machine Learning\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8AjqfdNIDg9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# GOFAI\n",
        "\n",
        "GOFAI refers to the early attempts at creating AI, remaining dominant until the mid 1980's. GOFAI is also referred to as \"Symbolic AI\", because it largely uses symbolic representations of problems and tries to logically determine an optimal solution or policy of behavior. GOFAI is considered a failed approach to AI, because it doesn't solve real-world problems without substantial symbolic interpretation by the person employing it.\n",
        "\n",
        "We'll cover a few examples of early GOFAI approaches to AI. For now we'll examine several **state-space search algorithms**, which try to find solutions for an environment with well-defined **states, transitions, and costs.**\n",
        "\n",
        "<br>Start by generating a 2D world below. Our agent will start at START, and is trying to reach the END. Its movements are limited to the selected movement type, and the cost of each movement is shown as the cost to move into that square."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgZOmnh6K2qo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Generate world]\n",
        "\n",
        "class World:\n",
        "  '''\n",
        "    Singleton class. The World class, once a world is generated, directly\n",
        "    holds a map of the world. All functions are static, and can be called\n",
        "    from the class.\n",
        "  '''\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__.update(kwargs)\n",
        "    World.world = self\n",
        "    for key in kwargs:\n",
        "      setattr(World, key, kwargs[key])\n",
        "\n",
        "  # Get the cost of moving to a specific position.\n",
        "  def get_cost(pos):\n",
        "    return World.world_map[pos[0]][pos[1]]\n",
        "\n",
        "  class Moves:\n",
        "    ROOK_MOVES = [(0,1),(1,0),(0,-1),(-1,0)]\n",
        "    KNIGHT_MOVES = [(a*i, b*j) for a,b in [(1,2),(2,1)] for i in (-1,1) for j in (-1,1)]\n",
        "    BISHOP_MOVES = [(i,j) for i in (-1,1) for j in (-1,1)]\n",
        "    KING_MOVES = ROOK_MOVES + BISHOP_MOVES\n",
        "\n",
        "  # Get the valid *positions* to move to from the current position.\n",
        "  def valid_moves(pos):\n",
        "    ret = []\n",
        "    for mov in World.moves:\n",
        "      new_pos = (pos[0]+mov[0], pos[1]+mov[1])\n",
        "      if new_pos[0] < 0 or new_pos[0] >= World.size or new_pos[1] < 0 or new_pos[1] >= World.size:\n",
        "        continue\n",
        "      ret.append(new_pos)\n",
        "    return ret\n",
        "\n",
        "  # Get the cost of a list of positions.\n",
        "  def cost(path):\n",
        "    tot = 0\n",
        "    for tile in path:\n",
        "      tot += World.get_cost(tile)\n",
        "    return tot\n",
        "  \n",
        "  # Create a new singleton world.\n",
        "  def generate_world(size, difficulty, moves):\n",
        "    world_map = [[max(1, random.gauss(1 + (difficulty), difficulty)) for _ in range(size)] for __ in range(size)]\n",
        "    world_map[0][0] = 1\n",
        "    world_map[size-1][size-1] = 1\n",
        "    World(size=size, world_map=world_map, moves=moves, start=(0,0), end=(size-1,size-1))\n",
        "\n",
        "  # Draw the current world into a new plt figure, or existing figure/image if provided.\n",
        "  def draw_world(ax=None, im=None):\n",
        "    if not ax or not im:\n",
        "      ax = plt.gca()\n",
        "      im = ax.matshow(World.world_map, cmap=plt.gray())\n",
        "      ax.figure.set_size_inches((0.8 * World.size,)*2)\n",
        "      plt.close(ax.figure)\n",
        "    else:\n",
        "      im.set_data(World.world_map)\n",
        "      im.set_clip_path(ax.patch)\n",
        "      im.set_extent([-0.5, World.size-0.5, World.size-0.5, -0.5])\n",
        "      im.autoscale()\n",
        "      ax.figure.set_size_inches((0.8 * World.size,)*2)\n",
        "      ax.xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(nbins=World.size+1, steps=[1,2,5], integer=True, prune=\"both\"))\n",
        "      ax.yaxis.set_major_locator(matplotlib.ticker.MaxNLocator(nbins=World.size+1, steps=[1,2,5], integer=True, prune=\"both\"))\n",
        "    return ax, im\n",
        "  \n",
        "  # Print utility to get string representation of the current world.\n",
        "  def raw_world():\n",
        "    return \"[[\" + \"],\\n [\".join(\", \".join(\"{:.1f}\".format(j) for j in i) for i in World.world_map) + \"]]\"\n",
        "\n",
        "  # Set up interactive widgets to generate a new world.\n",
        "  def setup_widgets():\n",
        "    def gen_world(b):\n",
        "      World.generate_world(size=size.value, difficulty=difficulty.value, moves=moves.value)\n",
        "      with output.use_tags('world_image'):\n",
        "        if hasattr(gen_world,\"ax\"):\n",
        "          World.draw_world(gen_world.ax, gen_world.im)\n",
        "        else:\n",
        "          ax, im = World.draw_world()\n",
        "          gen_world.ax = ax\n",
        "          gen_world.im = im\n",
        "        gen_world.ax.texts.clear()\n",
        "        for i in range(World.size):\n",
        "          for j in range(World.size):\n",
        "            if (i,j) != World.start and (i,j) != World.end:\n",
        "              gen_world.ax.annotate(\"{:.1f}\".format(World.world_map[i][j]), xy=(j,i), ha=\"center\", va=\"center\", color=\"r\")\n",
        "        gen_world.ax.annotate(\"START\", xy=World.start, ha=\"center\", va=\"center\", color=\"w\")\n",
        "        gen_world.ax.annotate(\"END\", xy=World.end, ha=\"center\", va=\"center\", color=\"w\")\n",
        "        if not gen_world.initialized:\n",
        "          gen_world.ax.figure.colorbar(mappable=gen_world.im, ax=gen_world.ax, fraction=0.042, pad=0.08, )\n",
        "          plt.show(block=False)\n",
        "          display(gen_world.ax.figure)\n",
        "          gen_world.initialized = True\n",
        "        else:\n",
        "          output.clear(wait=True, output_tags=('world_image',))\n",
        "          display(gen_world.ax.figure)\n",
        "\n",
        "    gen_world.initialized = False\n",
        "\n",
        "    moves = widgets.RadioButtons(options=[('Rook', World.Moves.ROOK_MOVES), \n",
        "                                          ('Knight', World.Moves.KNIGHT_MOVES), \n",
        "                                          ('Bishop', World.Moves.BISHOP_MOVES),\n",
        "                                          ('King', World.Moves.KING_MOVES)], \n",
        "                                  description=\"Movement type:\", value=World.Moves.ROOK_MOVES, disabled=False)\n",
        "    difficulty = widgets.FloatSlider(1, min=1.0, max=10.0, step=0.1, description=\"Difficulty:\")\n",
        "    size = widgets.IntSlider(8, 4, 12, description=\"Size:\")\n",
        "    param_box = widgets.HBox(children=(size, difficulty, moves))\n",
        "    world_output = widgets.Output()\n",
        "    gen_map = widgets.Button(description=\"Generate World\")\n",
        "    gen_map.on_click(gen_world)\n",
        "    display(param_box, gen_map, world_output)\n",
        "\n",
        "    display(widgets.HTML(value=\"\"\"<details><summary>Help:</summary><p>Generate a world for use in GOFAI examples. Each tile shows the cost of moving to that tile.<br>You can overwrite with a new random world using 'Generate World'.<ul><li>Size: set size of N by N board<li>Difficulty: increase variability of tile scores<li>Movement: set the available movement type for the board (like chess pieces)</ul></details>\"\"\"))\n",
        "    \n",
        "    gen_map.click()\n",
        "\n",
        "World.setup_widgets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYbZpokpKzRi",
        "colab_type": "text"
      },
      "source": [
        "## A* Search\n",
        "\n",
        "<img src=\"https://artint.info/figures/ch03/searchsp.gif\" width=\"400\">\n",
        "\n",
        "This search algorithm solves for the **optimal** path in a structured environment. A\\* requires a notion of **path cost** and a **heuristic** to estimate the remaining distance to a goal. The heuristic _must_ provide the minimum possible distance from any point to the goal. Then A\\* starts at the start and procedurally builds a **frontier** of paths to investigate and expand upon, until finally it comes upon the path reaching a goal node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKmo4C9-pb8m",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Show A* search]\n",
        "\n",
        "class AStarPath:\n",
        "  world_size = None\n",
        "  \n",
        "  def __init__(self, pos, hist, score=False):\n",
        "    AStarPath.init_knight_table()\n",
        "    self.pos = pos\n",
        "    self.hist = hist\n",
        "    self.g = World.cost(self)\n",
        "    self.h = AStarPath.heuristic(pos)\n",
        "    self.score = score or self.g + self.h\n",
        "  \n",
        "  def __iter__(self):\n",
        "    return iter(self.hist + [self.pos])\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    return self.pos[item]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return str([self.hist + [self.pos], self.score])\n",
        "\n",
        "  def init_knight_table():\n",
        "    if World.size != AStarPath.world_size:\n",
        "      AStarPath.world_size = World.size\n",
        "    else:\n",
        "      return\n",
        "    AStarPath.KNIGHT_TABLE = {World.end: 0}\n",
        "    for i in range(12):\n",
        "      starts = [key for key in AStarPath.KNIGHT_TABLE.keys() if AStarPath.KNIGHT_TABLE[key] == i-1]\n",
        "      for start in starts:\n",
        "        moves = [mov for mov in World.valid_moves(start) if mov not in AStarPath.KNIGHT_TABLE]\n",
        "        for mov in moves:\n",
        "          AStarPath.KNIGHT_TABLE[mov] = i\n",
        "    \n",
        "  def heuristic(pos):\n",
        "    move_type = World.moves\n",
        "    if move_type == World.Moves.ROOK_MOVES:\n",
        "      return 2*World.size - 2 - sum(pos)\n",
        "    elif move_type == World.Moves.BISHOP_MOVES or move_type == World.Moves.KING_MOVES:\n",
        "      return World.size-1 - min(pos)\n",
        "    elif move_type == World.Moves.KNIGHT_MOVES:\n",
        "      return AStarPath.KNIGHT_TABLE[pos]\n",
        "\n",
        "  def new_paths(self):\n",
        "    moves = World.valid_moves(self.pos)\n",
        "    return [AStarPath(mov, self.hist + [self.pos], self.score + World.get_cost(mov) - self.h + AStarPath.heuristic(mov)) for mov in moves]\n",
        "\n",
        "def astar_solve_world():\n",
        "  path_stack = [AStarPath(World.start, [])]\n",
        "  visited = []\n",
        "  path_count = 0\n",
        "  while path_stack:\n",
        "    path = path_stack.pop(0)\n",
        "    if path.pos in visited:\n",
        "      continue\n",
        "    if path.pos == World.end:\n",
        "      plt.pause(1.0 / display_frequency.value)\n",
        "      return path\n",
        "    path_count += 1\n",
        "    if path_count % display_interval.value == 0:\n",
        "      display_path(path)\n",
        "      plt.pause(1.0 / display_frequency.value)\n",
        "    visited.append(path.pos)\n",
        "    new_paths = path.new_paths()\n",
        "    for npath in new_paths:\n",
        "      if npath.pos in visited:\n",
        "        continue\n",
        "      i = next((j for j in range(len(path_stack)) if path_stack[j].score > npath.score), len(path_stack))\n",
        "      path_stack.insert(i, npath)\n",
        "  raise \"No solution found\"\n",
        "\n",
        "# This function maintains attributes .ax and .im to track its current plt figure.\n",
        "# This solves widget problems.\n",
        "def display_path(path, solved=False):\n",
        "  astar.ax.texts.clear()\n",
        "  astar.ax.annotate(\"START\", xy=World.start, ha=\"center\", va=\"center\", color=\"w\")\n",
        "  i = 1\n",
        "  for pos in path.hist[1:]:\n",
        "    astar.ax.annotate(str(i), xy=pos[::-1], ha=\"center\", va=\"center\", color=\"r\")\n",
        "    i += 1\n",
        "  if solved == True:\n",
        "    astar.ax.annotate(\"END\", xy=World.end, ha=\"center\", va=\"center\", color=\"w\")\n",
        "  output.clear(wait=True, output_tags='astar')\n",
        "  display(astar.ax.figure)\n",
        "  if solved:\n",
        "    display(path)\n",
        "  \n",
        "def astar(b):\n",
        "  output.clear(wait=True, output_tags='astar')\n",
        "  with output.use_tags('astar'):\n",
        "    astar.ax, astar.im = World.draw_world()\n",
        "    solution_path = astar_solve_world()\n",
        "    display_path(solution_path, solved=True)\n",
        "\n",
        "astar_button = widgets.Button(description=\"Run AStar\")\n",
        "display_interval = widgets.FloatLogSlider(value=4, base=2, min=1, max=6, step=1)\n",
        "display_frequency = widgets.FloatSlider(1, min=0.5, max=2)\n",
        "astar_button.on_click(astar)\n",
        "display(widgets.HBox((astar_button, widgets.HTML(\"<span style='display:inline-block;width:30px'></span>Refresh interval:\"), display_interval, widgets.HTML(\"<span style='display:inline-block;width:30px'></span>Refresh rate:\"), display_frequency)))\n",
        "display(widgets.HTML(value=\"\"\"<details><summary>Help:</summary><p>Run AStar to find the best path for the currently generated World.<li>Refresh interval: refresh the map with the current best path every N steps<li>Refresh rate: Time between refreshes</ul></details>\"\"\"))\n",
        "\n",
        "with output.use_tags('astar'):\n",
        "  output.clear(wait=True, output_tags='astar')\n",
        "  astar.ax, astar.im = World.draw_world()\n",
        "  display(astar.ax.figure)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oA779Zj9-AG",
        "colab_type": "text"
      },
      "source": [
        "## Reinforcement Learning\n",
        "\n",
        "Reinforcement learning falls in between GOFAI and machine learning. The reinforcement learning approach deliberately maims the agent by refusing to tell it the rules of the game. Instead, the agent is allowed to attempt an action, and the world will report to the agent the result of its action. The agent will slowly figure out how well an action worked from each state, and should eventually have a **policy** dictating the value of each action at each state. Then the agent should simply choose the best action available at each state.\n",
        "\n",
        "We'll specifically be using q-learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLvIsrMtA30A",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Show reinforcement learning]\n",
        "class QPolicy:\n",
        "  def __init__(self, explore_rate=0.2, learn_rate=0.25, discount_factor=0.98, reward=20, goal_bias=0.10):\n",
        "    self.q_map = {pos: {mov: 0 for mov in World.valid_moves(pos)} \n",
        "             for pos in [(i,j) for i in range(World.size) for j in range(World.size)]}\n",
        "    self.explore_rate = explore_rate\n",
        "    self.learn_rate = learn_rate\n",
        "    self.discount_factor=discount_factor\n",
        "    self.reward = reward\n",
        "    self.goal_bias=goal_bias\n",
        "    self._knight_init = False\n",
        "\n",
        "  def heuristic(self, pos):\n",
        "    def init_knight_table(self):\n",
        "      self.KNIGHT_TABLE = {World.end: 0}\n",
        "      for i in range(min(4, World.size) + 5):\n",
        "        starts = [key for key in self.KNIGHT_TABLE.keys() if self.KNIGHT_TABLE[key] == i-1]\n",
        "        for start in starts:\n",
        "          moves = [mov for mov in World.valid_moves(start) if mov not in self.KNIGHT_TABLE]\n",
        "          for mov in moves:\n",
        "            self.KNIGHT_TABLE[mov] = i\n",
        "    \n",
        "    move_type = World.moves\n",
        "    if move_type == World.Moves.ROOK_MOVES:\n",
        "      return 2*World.size - 2 - sum(pos)\n",
        "    elif move_type == World.Moves.BISHOP_MOVES or move_type == World.Moves.KING_MOVES:\n",
        "      return World.size-1 - min(pos)\n",
        "    elif move_type == World.Moves.KNIGHT_MOVES:\n",
        "      if not self._knight_init:\n",
        "        init_knight_table(self)\n",
        "        self._knight_init=True\n",
        "      return self.KNIGHT_TABLE[pos]\n",
        "\n",
        "  def get_q_value(self, pos, action):\n",
        "    return self.q_map[pos].get(action, -math.inf)\n",
        "\n",
        "  def best_action(self, pos):\n",
        "    best_choices = [act for act in self.q_map[pos] if math.isclose(self.get_q_value(pos, act), max(self.get_q_value(pos, act_) for act_ in self.q_map[pos]))]\n",
        "    if len(best_choices) == 1:\n",
        "      return best_choices[0]\n",
        "    else:\n",
        "      return None\n",
        "  \n",
        "  def best_path(self, start=(0,0)):\n",
        "    pos = start\n",
        "    path = []\n",
        "    while pos != World.end:\n",
        "      if pos in path:\n",
        "        return []\n",
        "      path.append(pos)\n",
        "      pos = self.exploit(pos)\n",
        "    return path + [World.end]\n",
        "\n",
        "  def adjust_q_value(self, pos, action, adjustment):\n",
        "    self.q_map[pos][action] += adjustment\n",
        "\n",
        "  def exploit(self, pos):\n",
        "    best_choices = [act for act in self.q_map[pos] if math.isclose(self.get_q_value(pos, act), max(self.get_q_value(pos, act_) for act_ in self.q_map[pos]))]\n",
        "    if len(best_choices) == 1:\n",
        "      return best_choices[0]\n",
        "    else:\n",
        "      return best_choices[np.random.randint(low=0, high=len(best_choices)-1)]\n",
        "\n",
        "  def explore(self, pos):\n",
        "    options = self.q_map[pos]\n",
        "    return list(options)[np.random.randint(low=0, high=len(options))]\n",
        "  \n",
        "  def episode(self):\n",
        "    path = []\n",
        "    pos = tuple(np.random.randint(low=0, high=World.size-1, size=2))\n",
        "    while pos != World.end:\n",
        "      path.append(pos)\n",
        "      do_exploit = (np.random.random() > self.explore_rate)\n",
        "      next_pos = self.exploit(pos) if do_exploit else self.explore(pos)\n",
        "      reward = self.reward if (next_pos == World.end) else -World.get_cost(next_pos) + self.goal_bias * (self.heuristic(pos) - self.heuristic(next_pos))\n",
        "      self.adjust_q_value(pos, next_pos, self.learn_rate * (reward + self.discount_factor * self.get_q_value(next_pos, max(self.q_map[next_pos], key=lambda x: self.get_q_value(next_pos, x))) - self.get_q_value(pos, next_pos)))\n",
        "      pos = next_pos\n",
        "    return path\n",
        "  \n",
        "  def display(self, ax=None, im=None):\n",
        "    if not ax or not im:\n",
        "      ax, im = World.draw_world()\n",
        "    for i in range(World.size):\n",
        "      for j in range(World.size):\n",
        "        if (i,j) == World.end:\n",
        "          ax.annotate(\"END\", xy=World.end, ha=\"center\", va=\"center\", color=\"w\")\n",
        "          continue\n",
        "        best = self.best_action((i,j))\n",
        "        if not best:\n",
        "          ax.annotate(\"?\", xy=(j,i), ha=\"center\", va=\"center\", color=\"r\")\n",
        "        else:\n",
        "          ax.annotate(\"\", xy=best[::-1], xytext=(j,i), \n",
        "                      arrowprops={\"color\": \"red\", \"arrowstyle\": \"->\", \"shrinkB\": 10}, ha=\"center\", va=\"center\", color=\"r\")\n",
        "    output.clear(wait=True, output_tags='q_learn')\n",
        "    display(ax.figure)\n",
        "\n",
        "widget_style = {\"description_width\": \"initial\"}\n",
        "q_widgets = {\"explore_rate\": widgets.FloatSlider(value=.2, min=0.05, max=0.5, step=0.05, description=\"Explore rate\", style=widget_style),\n",
        "             \"learn_rate\": widgets.FloatSlider(value=0.25, min=0.05, max=0.5, step=0.05, description=\"Learn rate\", style=widget_style),\n",
        "             \"discount_factor\": widgets.FloatSlider(value=0.98, min=0.8, max=1.0, step=0.01, description=\"Discount factor\", style=widget_style),\n",
        "             \"reward\": widgets.IntSlider(value=20, min=5, max=200, step=5, description=\"Reward\", style=widget_style),\n",
        "             \"goal_bias\": widgets.FloatSlider(value=0.10, min=0.0, max=0.20, step=0.01, description=\"Goal bias\", style=widget_style),\n",
        "             \"learn_box\": widgets.HBox(children=(\n",
        "                widgets.Button(description=\"Run Q-Learning\"),\n",
        "                widgets.IntText(value=500, step=100, description=\"# of episodes:\", style=widget_style),\n",
        "                widgets.Checkbox(value=False, description=\"Show progress? (SLOW)\", style=widget_style))),\n",
        "             \"episode_box\": widgets.HBox(children=(\n",
        "                widgets.Button(description=\"Run single episode\"),\n",
        "                widgets.IntSlider(value=1, min=1, max=100, step=1, description=\"Episode #:\", style=widget_style))),\n",
        "             }\n",
        "\n",
        "def q_learn(b):\n",
        "  q = QPolicy(**{a: q_widgets.get(a).value for a in [\"explore_rate\", \"learn_rate\", \"discount_factor\", \"reward\", \"goal_bias\"]})\n",
        "  num_episodes = q_widgets.get(\"learn_box\").children[1].value\n",
        "  with output.use_tags('q_learn'):\n",
        "    for i in range(num_episodes):\n",
        "      q.episode()\n",
        "      if q_widgets[\"learn_box\"].children[2].value and (i+1) % (math.floor(num_episodes / 10.0)) == 0:\n",
        "        q.display()\n",
        "        time.sleep(1)\n",
        "    q.display()\n",
        "    best_path = q.best_path()\n",
        "    if best_path:\n",
        "      print(best_path, World.cost(best_path))\n",
        "    else:\n",
        "      print(\"[!] Policy contains inescapable recursive loop.\")\n",
        "\n",
        "def q_episode(b):\n",
        "  q = QPolicy(**{a: q_widgets.get(a).value for a in [\"explore_rate\", \"learn_rate\", \"discount_factor\", \"reward\", \"goal_bias\"]})\n",
        "  for _ in range(q_widgets[\"episode_box\"].children[1].value):\n",
        "    q.episode()\n",
        "  path = q.episode()\n",
        "  with output.use_tags('q_learn'):\n",
        "    output.clear(wait=True, output_tags='q_learn')\n",
        "    ax, im = World.draw_world()\n",
        "    ax.annotate(\"\", xy=path[1][::-1], xytext=path[0][::-1], arrowprops={\"color\": \"red\", \"arrowstyle\": \"->\", \"shrinkB\": 10}, ha=\"center\", va=\"center\")\n",
        "    for i in range(1,len(path) - 1):\n",
        "      ax.annotate(\"\", xy=path[i+1][::-1], xytext=path[i][::-1], arrowprops={\"color\": \"#{0:02x}{1:02x}{2:02x}\".format(5*i % 256, (50 + 3*i) % 256, (80 + i) % 256), \"arrowstyle\": \"->\", \"shrinkB\": 10}, ha=\"center\", va=\"center\")\n",
        "    ax.annotate(\"\", xy=World.end, xytext=path[-1][::-1], arrowprops={\"color\": \"red\", \"arrowstyle\": \"->\", \"shrinkB\": 10}, ha=\"center\", va=\"center\")\n",
        "    ax.annotate(\"S\", xy=path[0][::-1], va=\"center\", ha=\"center\", color=\"#ff33ff\")\n",
        "    output.clear(wait=True, output_tags='q_learn')\n",
        "    display(ax.figure)\n",
        "\n",
        "q_widgets[\"learn_box\"].children[0].on_click(q_learn)\n",
        "q_widgets[\"episode_box\"].children[0].on_click(q_episode)\n",
        "\n",
        "display(*q_widgets.values())\n",
        "display(widgets.HTML(value=\"\"\"\\\n",
        "<details><summary>Help:</summary>\n",
        "<p>Create a policy for the current world by using <u>Run Q-Learning</u>.<br>\n",
        "View what the agent does in a single episode by using <u>Run single episode</u>, using the <u>Episode #</u> slider to see the Nth episode.\n",
        "<ul>\n",
        "  <li>Explore rate: set the rate at which the agent chooses a random direction instead of the current best.\n",
        "  <li>Learn rate: set how quickly the agent gains knowledge of the results of each action.\n",
        "  <li>Discount factor: set the discount factor for each successive tile. The lower this is, the more the agent is punished for taking a long time to get to the goal.\n",
        "  <li>Reward: set the reward level for reaching the goal tile. This reward is what teaches the agent that moving towards the goal is a good thing. This generally should scale with the board difficulty and size.\n",
        "  <li>Goal bias: this bias gives the agent a slight immediate reward for heading in the direction of the goal. This speeds up the initial episodes by preventing the agent from going in circles at random.\n",
        "  <li># of Episodes: the agent is dropped into the world and explores or progresses towards the goal this many times. Note that for a board with 100 tiles, 100 episodes would mean several tiles are likely not to ever be started on.\n",
        "</ul></details>\\\n",
        "\"\"\"))\n",
        "\n",
        "with output.use_tags('q_learn'):\n",
        "  ax, im = World.draw_world()\n",
        "  display(ax.figure)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky5Hux3VWjDz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Machine Learning\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f0499405-1197-4b43-b7c5-40548eeb9f34/Image/5f0b569126b3f2da0538b3ce34bd89d2/difference_between_ai__machine_learning_and_deep_learning.png\" width=\"800\">\n",
        "\n",
        "Machine learning followed the decline of GOFAI by relying on new *data driven* methodologies. Machine learning is often wrongly conflated with \"artificial intelligence\", \"deep learning\", \"neural networks\". Here's how to read those terms:\n",
        "- **Machine learning**: the methodology in artificial intelligence which infers patterns and rules from natural data, but chiefly not from pre-determined rules. These algorithms create a model that can be applied on new data-sets.\n",
        "- **Deep Learning**: the class of machine learning algorithms which use multiple layers, each tuned to solving a sub-problem or learning specific problems, to extract higher level features from an input.\n",
        "- **Neural Networks**: an application of deep learning which connects layers of nodes in a network, where the entire network is trained on a problem together. Each layer of nodes takes a previous layer of nodes in the network as input, and typically each node provides a single output (usually between -1 and 1).\n",
        "\n",
        "<br><br><br>\n",
        "<img src=\"https://miro.medium.com/max/775/1*Qn4eJPhkvrEQ62CtmydLZw.png\" width=\"600\">\n",
        "\n",
        "Further, machine learning algorithms are split into two types:\n",
        "- **Regression**: these algorithms provide continuous output, usually consisting of a set of outputs totaling 1. These outputs usually represent the confidence in an assessment, e.g. 70% (or 0.7) chance of rain.\n",
        "- **Classification**: these algorithms apply discrete labels, determining what type of pattern is identified in an input, e.g. \"This is a hot dog / this is not a hot dog.\" Often, a classification model can be derived from a regression model by simply classifying a sample according to a highest scoring regression model.\n",
        "\n",
        "<br><br><br>\n",
        "<img src=\"https://www.researchgate.net/publication/329533120/figure/fig1/AS:702267594399761@1544445050584/Supervised-learning-and-unsupervised-learning-Supervised-learning-uses-annotation.png\">\n",
        "\n",
        "*Further*, these algorithms are trained in one of two ways:\n",
        "- **Supervised**: data provided to the agent is labeled, so that the model associates patterns to specific labels.\n",
        "- **Unsupervised**: data is *not* labeled. The agent assesses a data-set and begins to group similar samples, identifying patterns. This provides a model which assesses new data to be 'like' previously identified patterns or groups of data.\n",
        "- **Reincorcement learning**: data is *created* by the agent. This is considered machine learning because it is driven by data, but counts as GOFAI because the data is artificial and can operate in purely theoretical domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6sz1CeDJV15",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Import MNIST dataset]\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "(x_train, x_test) = (x_train / 255.0, x_test / 255.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bwsjTl4b-wH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Create plot utilities]\n",
        "def plot_image(predictions_array, true_label, img):\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predictions_array = predictions_array / sum(predictions_array)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = '#D0D0D0'\n",
        "  else:\n",
        "    color = '#D08080'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n",
        "                                100*np.max(predictions_array),\n",
        "                                true_label),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(predictions_array, true_label):\n",
        "  predictions_array = predictions_array / sum(predictions_array)\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\", )\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AvzaDtOSleA",
        "colab_type": "text"
      },
      "source": [
        "Let's examine the MNIST dataset. This dataset has a large number of 28x28 sized grayscale images of hand-written digits, 0-9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSYthK09TQmr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Show image examples]\n",
        "plt.figure(figsize=(10,2))\n",
        "for i in range(5):\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.imshow(x_test[i+5], cmap=plt.cm.binary)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel(\"{}\".format(y_test[i+5]), color=\"#D0D0D0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRdqBgLNUfqh",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll create, compile, and train 4 models using Keras with TensorFlow:\n",
        "- Best of 10 1-node linear regression models\n",
        "- Best of 10 1-node logistic regression models\n",
        "- Neural network classification with 140-node linear hidden layer\n",
        "- Neural network classification with 140-node logistic hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY7PAzTtH-wW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Create models]\n",
        "linear_model = keras.Sequential([\n",
        "                                 keras.layers.Flatten(input_shape=(28,28)), \n",
        "                                 keras.layers.Dense(10, activation='relu', bias_initializer=keras.initializers.RandomNormal()),\n",
        "                                 keras.layers.Lambda(lambda x: x**2)\n",
        "                                 ])\n",
        "\n",
        "logistic_model = keras.Sequential([\n",
        "                                   keras.layers.Flatten(input_shape=(28,28)), \n",
        "                                   keras.layers.Dense(10, activation='sigmoid', bias_initializer=keras.initializers.RandomNormal()),\n",
        "                                   keras.layers.Lambda(lambda x: x**2)\n",
        "                                   ])\n",
        "\n",
        "neural_linear_model = keras.Sequential([\n",
        "                                        keras.layers.Flatten(input_shape=(28,28)), \n",
        "                                        keras.layers.Dense(140, activation='relu', bias_initializer=keras.initializers.RandomNormal()), \n",
        "                                        keras.layers.Dense(10, activation='softmax')\n",
        "                                        ])\n",
        "\n",
        "neural_logistic_model = keras.Sequential([\n",
        "                                          keras.layers.Flatten(input_shape=(28,28)), \n",
        "                                          keras.layers.Dense(140, activation='sigmoid', bias_initializer=keras.initializers.RandomNormal()), \n",
        "                                          keras.layers.Dense(10, activation='softmax')\n",
        "                                          ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mb_NxoWoINU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Compile models]\n",
        "linear_model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0001), \n",
        "                     loss=keras.losses.SparseCategoricalCrossentropy(), \n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "logistic_model.compile(optimizer=keras.optimizers.SGD(), \n",
        "                       loss=keras.losses.SparseCategoricalCrossentropy(), \n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "neural_linear_model.compile(optimizer=keras.optimizers.SGD(), \n",
        "                            loss=keras.losses.SparseCategoricalCrossentropy(), \n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "neural_logistic_model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001), \n",
        "                              loss=keras.losses.SparseCategoricalCrossentropy(), \n",
        "                              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8wAsfvVqNz7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Train models]\n",
        "num_epochs = 5 #@param {\"type\": \"slider\", \"min\": 1, \"max\": 25}\n",
        "linear_model.fit(x_train, y_train, epochs=num_epochs)\n",
        "logistic_model.fit(x_train, y_train, epochs=num_epochs)\n",
        "neural_linear_model.fit(x_train, y_train, epochs=num_epochs)\n",
        "neural_logistic_model.fit(x_train, y_train, epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW6uqHRI3Eox",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Test samples]\n",
        "num_samples = 20 #@param {'type': 'slider', 'min': 1, 'max': 100}\n",
        "sample_indices = np.random.choice(x_test.shape[0], size=num_samples, replace=False)\n",
        "x_sample = np.array([x_test[i] for i in sample_indices])\n",
        "y_sample = np.array([y_test[i] for i in sample_indices])\n",
        "lin_predicts = linear_model.predict(x_sample)\n",
        "log_predicts = logistic_model.predict(x_sample)\n",
        "neural_lin_predicts = neural_linear_model.predict(x_sample)\n",
        "neural_log_predicts = neural_logistic_model.predict(x_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wivCyzebe3VI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Linear model predictions\n",
        "sampnum_5 = math.ceil(num_samples / 5.0)\n",
        "plt.figure(figsize=(20,2*sampnum_5))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(sampnum_5,10,2*i+1)\n",
        "    plot_image(lin_predicts[i], y_sample[i], x_sample[i])\n",
        "    plt.subplot(sampnum_5,10,2*i+2)\n",
        "    plot_value_array(lin_predicts[i], y_sample[i])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MDreInzdbPA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Logistic model predictions\n",
        "sampnum_5 = math.ceil(num_samples / 5.0)\n",
        "plt.figure(figsize=(20,2*sampnum_5))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(sampnum_5,10,2*i+1)\n",
        "    plot_image(log_predicts[i], y_sample[i], x_sample[i])\n",
        "    plt.subplot(sampnum_5,10,2*i+2)\n",
        "    plot_value_array(log_predicts[i], y_sample[i])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y-u0YX9PKNc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Neural linear model predictions\n",
        "sampnum_5 = math.ceil(num_samples / 5.0)\n",
        "plt.figure(figsize=(20,2*sampnum_5))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(sampnum_5,10,2*i+1)\n",
        "    plot_image(neural_lin_predicts[i], y_sample[i], x_sample[i])\n",
        "    plt.subplot(sampnum_5,10,2*i+2)\n",
        "    plot_value_array(neural_lin_predicts[i], y_sample[i])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYPipz5PKTj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Neural logistic model predictions\n",
        "sampnum_5 = math.ceil(num_samples / 5.0)\n",
        "plt.figure(figsize=(20,2*sampnum_5))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(sampnum_5,10,2*i+1)\n",
        "    plot_image(neural_log_predicts[i], y_sample[i], x_sample[i])\n",
        "    plt.subplot(sampnum_5,10,2*i+2)\n",
        "    plot_value_array(neural_log_predicts[i], y_sample[i])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyDAqkyvmKeA",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Playground\n",
        "\n",
        "Experiment in the playground to learn more about how neural networks work, or [go to tensorflow's website to view more.](https://playground.tensorflow.org)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXVr7DFYlq__",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown [Load TensorFlow Playground]\n",
        "%%html\n",
        "<iframe width=\"1400\" height=\"1000\" name=\"tf_playground\" src=\"https://playground.tensorflow.org#activation=sigmoid\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9KGJsd_rf0K",
        "colab_type": "text"
      },
      "source": [
        "##Exercises\n",
        "Run each exercise to open its specialized playground."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTeDVJjir9uw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown ####What biases predict this dataset well with no hidden layers?\n",
        "#@markdown <details><summary>Hint:</summary><p>What line would you use? What's the <i>slope</i> of that line?<p>(Pay careful attention to how x and y are oriented on the graph.)</details>\n",
        "\n",
        "%%html\n",
        "<iframe width=\"1400\" height=\"700\" src=\"https://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.0001&regularizationRate=0&noise=0&networkShape=&seed=0.55382&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=true&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true&numHiddenLayers_hide=true&discretize_hide=true&activation_hide=true&problem_hide=true&noise_hide=true&regularization_hide=true&dataset_hide=true&batchSize_hide=true&playButton_hide=false\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCBaGwzuwKH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown ####With 1 hidden layer, how many neurons are required to get a decent model?\n",
        "#@markdown <details><summary>Hint:</summary><p>If you were drawing lines to capture the inner circle, how many lines would it take to enclose it?</details>\n",
        "\n",
        "%%html\n",
        "<iframe width=\"1400\" height=\"700\" src=\"https://playground.tensorflow.org/#activation=sigmoid&regularization=L2&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.84062&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&showTestData_hide=true&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true&numHiddenLayers_hide=true&discretize_hide=true&activation_hide=true&problem_hide=true&noise_hide=true&regularization_hide=true&dataset_hide=true&batchSize_hide=true&playButton_hide=false\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA7Ukyxkxuhl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown ####With no hidden layers, which 2 inputs can produce a decent model?\n",
        "#@markdown <details><summary>Hint:</summary><p>Do you remember the equation for a circle?</details>\n",
        "\n",
        "%%html\n",
        "<iframe width=\"1400\" height=\"700\" src=\"https://playground.tensorflow.org/#activation=sigmoid&regularization=L2&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.84062&showTestData=false&discretize=false&percTrainData=50&x=false&y=false&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=true&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true&numHiddenLayers_hide=true&discretize_hide=true&activation_hide=true&problem_hide=true&noise_hide=true&regularization_hide=true&dataset_hide=true&batchSize_hide=true&playButton_hide=false\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ag8xmuy-Vbp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown ####Let this model run. Pay attention to the test loss and training loss. Is the model improving? Why or why not?\n",
        "#@markdown <details><summary>Hint:</summary><p>The data shown in the picture is the <i>training data</i>. How well does it represent the real data this model would be tested on?<p>(Also take note of the noise level for this dataset.)</details>\n",
        "\n",
        "%%html\n",
        "<iframe width=\"1400\" height=\"700\" src=\"https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=30&networkShape=6,4,3&seed=0.94343&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&dataset_hide=true&regularization_hide=true&batchSize_hide=true&problem_hide=true&activation_hide=true&regularizationRate_hide=true&percTrainData_hide=false&numHiddenLayers_hide=true&learningRate_hide=true\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxrHAEPBxufS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown ####How much wood would a woodchuck chuck if a woodchuck could chuck wood?\n",
        "#@markdown <details><summary>Hint:</summary><p>A woodchuck would chuck as much wood as a woodchuck could chuck if a woodchuck could chuck wood.</details>\n",
        "#@markdown <p>(No exercise - this is just an interesting neural network.)\n",
        "%%html\n",
        "<iframe width=\"1400\" height=\"700\" src=\"https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,8,5&seed=0.53586&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&showTestData_hide=true&activation_hide=true&problem_hide=true&noise_hide=true&discretize_hide=true&regularization_hide=true&dataset_hide=true&batchSize_hide=true&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true&numHiddenLayers_hide=true\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}